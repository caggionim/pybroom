{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyBroom Example - Multiple Datasets - Scipy Robust Fit\n",
    "\n",
    "*This notebook is part of* [pybroom](https://github.com/tritemio/pybroom).\n",
    "\n",
    ">This notebook demonstrate using *pybroom* when fitting **a set of curves** (curve fitting) using robust fitting and scipy.\n",
    ">We will show that *pybroom* greatly simplifies comparing, filtering and plotting fit results \n",
    ">from multiple datasets.\n",
    "> See\n",
    ">[pybroom-example-multi-datasets](pybroom-example-multi-datasets.ipynb)\n",
    "> for an example using `lmfit.Model` instead of directly scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'  # for hi-dpi displays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import normpdf\n",
    "import seaborn as sns\n",
    "from lmfit import Model\n",
    "import lmfit\n",
    "print('lmfit: %s' % lmfit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pybroom as br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Noisy Data\n",
    "\n",
    "We start simulating *N* datasets which are identical except for the additive noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10, 10, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peak1 = lmfit.models.GaussianModel(prefix='p1_')\n",
    "peak2 = lmfit.models.GaussianModel(prefix='p2_')\n",
    "model = peak1 + peak2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#params = model.make_params(p1_amplitude=1.5, p2_amplitude=1, \n",
    "#                           p1_sigma=1, p2_sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_data = np.zeros((N, x.size))\n",
    "Y_data.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(Y_data.shape[0]):\n",
    "    Y_data[i] = model.eval(x=x, p1_center=-1, p2_center=2, \n",
    "                           p1_sigma=0.5, p2_sigma=1, \n",
    "                           p1_height=1, p2_height=0.5)\n",
    "Y_data += np.random.randn(*Y_data.shape)/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_outliers = int(Y_data.size * 0.05)\n",
    "idx_ol = np.random.randint(low=0, high=Y_data.size, size=num_outliers)\n",
    "Y_data.reshape(-1)[idx_ol] = (np.random.rand(num_outliers) - 0.5)*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x, Y_data.T, 'ok', alpha=0.1);\n",
    "plt.title('%d simulated datasets, with outliers' % N);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### curve_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model PDF to be maximized\n",
    "def model_pdf(x, a1, a2, mu1, mu2, sig1, sig2):\n",
    "    return (a1 * normpdf(x, mu1, sig1) + \n",
    "            a2 * normpdf(x, mu2, sig2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = so.curve_fit(model_pdf, x, Y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(result), type(result[0]), type(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a `namedtuple` is a clean way to assign names to an array of paramenters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Params = namedtuple('Params', 'a1 a2 mu1 mu2 sig1 sig2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = Params(*result[0])\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, not much data is returned by `curve_fit`, a 2-element tuple with:\n",
    "\n",
    "- array of best-fit parameters\n",
    "- array of jacobian\n",
    "\n",
    "Therefore `curve_fit` is not very useful for detailed comparison of fit results. \n",
    "A better interface for curve fitting would be *lmfit.Model* (see \n",
    "[this other notebook](pybroom-example-multi-datasets.ipynb)).\n",
    "\n",
    "In the current notebook we keep exploring further options offered by `scipy.optimize`.\n",
    "\n",
    "\n",
    "### least_squares()\n",
    "\n",
    "As an example, we use the `least_squares` function which supports robust loss functions and constraints.\n",
    "\n",
    "We need to define the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residuals(p, x, y):\n",
    "    return y - model_pdf(x, *p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we fit the *N* datasets with different loss functions storing result in a dict containing lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "losses = ('linear', 'huber', 'cauchy')\n",
    "Results = {}\n",
    "for loss in losses:\n",
    "    Results[loss] = [so.least_squares(residuals, (1,1,0,1,1,1), args=(x, y), loss=loss, f_scale=0.5)\n",
    "                     for y in Y_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**NOTE**: For more details on robust fitting and on the different loss functions see\n",
    "[Robust nonlinear regression in scipy](http://scipy-cookbook.readthedocs.io/items/robust_regression.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# result = Results['cauchy'][0]\n",
    "# for k in result.keys():\n",
    "#     print(k, type(result[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying the results\n",
    "\n",
    "Now we tidy the results, combining the results for the different loss functions\n",
    "in a single DataFrames.\n",
    "\n",
    "We start with the `glance` function, which returns one row per fit result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg_tot = br.glance(Results, var_names=['loss', 'dataset'])\n",
    "dg_tot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg_tot.success.all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply `tidy`, which returns one row per parameter.\n",
    "\n",
    "Since the object `OptimzeResult` returned by `scipy.optimize` does \n",
    "only contains an array of parameters, we need to pass the names as\n",
    "as additional argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pnames = 'a1 a2 mu1 mu2 sig1 sig2'\n",
    "dt_tot = br.tidy(Results, var_names=['loss', 'dataset'], param_names=pnames)\n",
    "dt_tot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we cannot apply the\n",
    "`augment` function, since the `OptimizeResult` object\n",
    "does not include much per-data-point information \n",
    "(it may contain the array of residuals)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we plot the peak position and sigmas distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kws = dict(bins = np.arange(-2, 4, 0.1), histtype='step', lw=2)\n",
    "for loss in losses:\n",
    "    dt_tot.query('(name == \"mu1\" or name == \"mu2\") and loss == \"%s\"' % loss)['value'].hist(label=loss, **kws)\n",
    "    kws['ax'] = plt.gca()\n",
    "plt.title(' Distribution of peaks centers')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kws = dict(bins = np.arange(0, 4, 0.1), histtype='step', lw=2)\n",
    "for loss in losses:\n",
    "    dt_tot.query('(name == \"sig1\" or name == \"sig2\") and loss == \"%s\"' % loss)['value'].hist(label=loss, **kws)\n",
    "    kws['ax'] = plt.gca()\n",
    "plt.title(' Distribution of peaks sigmas')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complete overview for all the fit paramenters can be obtained with a factorplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.factorplot(x='loss', y='value', data=dt_tot, col='name', hue='loss', \n",
    "               col_wrap=4, kind='point', sharey=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the previous plots we see that, as espected, using robust fitting \n",
    "with higher damping of outlier (i.e. `cauchy` vs `huber` or `linear`) \n",
    "results in more accurate fit results.\n",
    "\n",
    "Finally, we can have a peek at the comparison of raw data and fitted models\n",
    "for a few datatsets.\n",
    "\n",
    "Since `OptimizeResults` does not include \"augmented\" data we need to \n",
    "generate these data by evaluating the model with the best-fit parameters.\n",
    "We use seaborn's `FacetGrid`, passing a custom function `_plot`\n",
    "for model evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _plot(names, values, x, label=None, color=None):\n",
    "    df = pd.concat([names, values], axis=1)\n",
    "    kw_pars = br.tidy_to_dict(df)\n",
    "    y = model_pdf(x, **kw_pars)\n",
    "    plt.plot(x, y, lw=2, color=color, label=label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = sns.FacetGrid(dt_tot.query('dataset < 9'), col='dataset', hue='loss', col_wrap=3)\n",
    "grid.map(_plot, 'name', 'value', x=x)\n",
    "grid.add_legend()\n",
    "for i, ax in enumerate(grid.axes):\n",
    "    ax.plot(x, Y_data[i], 'o', ms=3, color='k')\n",
    "plt.ylim(-1, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, the `ModelResult` object returned by lmfit,\n",
    "contains not only the evaluated model but also the evaluation\n",
    "of the single components (each single peak in this case).\n",
    "Therefore the above plot can be generated more straighforwardly\n",
    "using the \"augmented\" data.\n",
    "See the notebook [pybroom-example-multi-datasets](pybroom-example-multi-datasets.ipynb)\n",
    "for an example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
