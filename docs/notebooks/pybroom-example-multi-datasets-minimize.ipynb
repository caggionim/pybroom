{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyBroom Example - Multiple Datasets - Minimize\n",
    "\n",
    "*This notebook is part of* [pybroom](https://github.com/tritemio/pybroom).\n",
    "\n",
    ">This notebook demonstrate using pybroom when performing **Maximum-Likelihood fitting**\n",
    ">(scalar minimization as opposed to curve fitting) of a set of datasets with *lmfit.minimize*.\n",
    ">We will show that *pybroom* greatly simplifies comparing, filtering and plotting fit results \n",
    ">from multiple datasets.\n",
    ">For an example using curve fitting see\n",
    ">[pybroom-example-multi-datasets](pybroom-example-multi-datasets.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'  # for hi-dpi displays\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import normpdf\n",
    "import seaborn as sns\n",
    "from lmfit import Model\n",
    "import lmfit\n",
    "print('lmfit: %s' % lmfit.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pybroom as br"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Noisy Data\n",
    "\n",
    "Simulate *N* datasets which are identical except for the additive noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 20  # number of datasets\n",
    "n = 1000  # number of sample in each dataset\n",
    "\n",
    "np.random.seed(1)\n",
    "d1 = np.random.randn(20, int(0.6*n))*0.5 - 2\n",
    "d2 = np.random.randn(20, int(0.4*n))*1.5 + 2\n",
    "d = np.hstack((d1, d2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(data=d, columns=range(d.shape[1])).stack().reset_index()\n",
    "ds.columns = ['dataset', 'sample', 'data']\n",
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kws = dict(bins = np.arange(-5, 5.1, 0.1), histtype='step', \n",
    "           lw=2, color='k', alpha=0.1)\n",
    "for i in range(N):\n",
    "    ds.loc[ds.dataset == i, :].data.plot.hist(**kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-peaks model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use a Gaussian mixture distribution for fitting the data.\n",
    "\n",
    "We fit the data using the Maximum-Likelihood method, i.e. we minimize the\n",
    "(negative) log-likelihood function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model PDF to be maximized\n",
    "def model_pdf(x, a2, mu1, mu2, sig1, sig2):\n",
    "    a1 = 1 - a2\n",
    "    return (a1 * normpdf(x, mu1, sig1) + \n",
    "            a2 * normpdf(x, mu2, sig2))\n",
    "\n",
    "# Function to be minimized by lmfit\n",
    "def log_likelihood_lmfit(params, x):\n",
    "    pnames = ('a2', 'mu1', 'mu2', 'sig1', 'sig2')\n",
    "    kws = {n: params[n].value for n in pnames}\n",
    "    return -np.log(model_pdf(x, **kws)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters and \"fit\" the $N$ datasets by minimizing the (scalar) function `log_likelihood_lmfit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lmfit.Parameters()\n",
    "params.add('a2', 0.5, min=0, max=1)\n",
    "params.add('mu1', -1, min=-5, max=5)\n",
    "params.add('mu2', 1, min=-5, max=5)\n",
    "params.add('sig1', 1, min=1e-6)\n",
    "params.add('sig2', 1, min=1e-6)\n",
    "params.add('ax', expr='a2')   # just a test for a derived parameter\n",
    "\n",
    "Results = [lmfit.minimize(log_likelihood_lmfit, params, args=(di,), \n",
    "                          nan_policy='omit', method='least_squares')\n",
    "           for di in d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit results can be inspected with\n",
    "`lmfit.fit_report()` or `params.pretty_print()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lmfit.fit_report(Results[0]))\n",
    "print()\n",
    "Results[0].params.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good for peeking at the results. However,\n",
    "extracting these data from lmfit objects is quite a chore\n",
    "and requires good knowledge of lmfit objects structure.\n",
    "\n",
    "**pybroom** helps in this task: it extracts data from fit results and\n",
    "returns familiar pandas DataFrame (in tidy format). \n",
    "Thanks to the tidy format these data can be\n",
    "much more easily manipulated, filtered and plotted.\n",
    "\n",
    "Let's use the [glance](http://pybroom.readthedocs.io/en/latest/api.html#pybroom.glance) and \n",
    "[tidy](http://pybroom.readthedocs.io/en/latest/api.html#pybroom.tidy) functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg = br.glance(Results)\n",
    "dg.drop('message', 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt = br.tidy(Results, var_name='dataset')\n",
    "dt.query('dataset == 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while glance returns one row per fit result, the tidy function\n",
    "return one row per fitted parameter.\n",
    "\n",
    "We can query the value of one parameter (peak position) across the multiple datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.query('name == \"mu1\"').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By computing the standard deviation of the peak positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.query('name == \"mu1\"')['value'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.query('name == \"mu2\"')['value'].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the estimation of `mu1` as less error than the estimation\n",
    "of `mu2`. \n",
    "\n",
    "This difference can be also observed in the histogram of \n",
    "the fitted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt.query('name == \"mu1\"')['value'].hist()\n",
    "dt.query('name == \"mu2\"')['value'].hist(ax=plt.gca());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use pybroom's [tidy_to_dict](http://pybroom.readthedocs.io/en/latest/api.html#pybroom.tidy_to_dict) \n",
    "and [dict_to_tidy](http://pybroom.readthedocs.io/en/latest/api.html#pybroom.dict_to_tidy) \n",
    "functions to convert\n",
    "a set of fitted parameters to a dict (and vice-versa):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kwd_params = br.tidy_to_dict(dt.loc[dt['dataset'] == 0])\n",
    "kwd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "br.dict_to_tidy(kwd_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conversion is useful to call a python functions\n",
    "passing argument values from a tidy DataFrame. \n",
    "\n",
    "For example, here we use `tidy_to_dict`\n",
    "to easily plot the model distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.arange(-5, 5.01, 0.25)\n",
    "x = bins[:-1] + 0.5*(bins[1] - bins[0])\n",
    "grid = sns.FacetGrid(ds.query('dataset < 6'), col='dataset', hue='dataset', col_wrap=3)\n",
    "grid.map(plt.hist, 'data', bins=bins, normed=True);\n",
    "for i, ax in enumerate(grid.axes):\n",
    "    kw_pars = br.tidy_to_dict(dt.loc[dt.dataset == i], keys_exclude=['ax'])\n",
    "    y = model_pdf(x, **kw_pars)\n",
    "    ax.plot(x, y, lw=2, color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-peak model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of the example we also fit the $N$ datasets with a single Gaussian distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_pdf1(x, mu, sig):\n",
    "    return normpdf(x, mu, sig)\n",
    "\n",
    "def log_likelihood_lmfit1(params, x):\n",
    "    return -np.log(model_pdf1(x, **params.valuesdict())).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lmfit.Parameters()\n",
    "params.add('mu', 0, min=-5, max=5)\n",
    "params.add('sig', 1, min=1e-6)\n",
    "\n",
    "Results1 = [lmfit.minimize(log_likelihood_lmfit1, params, args=(di,), \n",
    "                          nan_policy='omit', method='least_squares')\n",
    "           for di in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dg1 = br.glance(Results)\n",
    "dg1.drop('message', 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt1 = br.tidy(Results1, var_name='dataset')\n",
    "dt1.query('dataset == 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augment?\n",
    "\n",
    "Pybroom [augment](http://pybroom.readthedocs.io/en/latest/api.html#pybroom.augment) function \n",
    "extracts information that is the same size as the input dataset,\n",
    "for example the array of residuals. In this case, however, we performed a scalar minimization\n",
    "(the log-likelihood function returns a scalar) and therefore the `MinimizerResult` object\n",
    "does not contain any residual array or other data of the same size as the dataset.\n",
    "\n",
    "## Comparing fit results\n",
    "\n",
    "We will do instead a comparison of single and two-peaks distribution using the results\n",
    "from the `tidy` function obtained in the previous section.\n",
    "\n",
    "We start with the following plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt['model'] = 'twopeaks'\n",
    "dt1['model'] = 'onepeak'\n",
    "dt_tot = pd.concat([dt, dt1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.arange(-5, 5.01, 0.25)\n",
    "x = bins[:-1] + 0.5*(bins[1] - bins[0])\n",
    "grid = sns.FacetGrid(ds.query('dataset < 6'), col='dataset', hue='dataset', col_wrap=3)\n",
    "grid.map(plt.hist, 'data', bins=bins, normed=True);\n",
    "for i, ax in enumerate(grid.axes):\n",
    "    kw_pars = br.tidy_to_dict(dt_tot.loc[(dt_tot.dataset == i) & (dt_tot.model == 'onepeak')])\n",
    "    y1 = model_pdf1(x, **kw_pars)\n",
    "    li1, = ax.plot(x, y1, lw=2, color='k', alpha=0.5)\n",
    "    kw_pars = br.tidy_to_dict(dt_tot.loc[(dt_tot.dataset == i) & (dt_tot.model == 'twopeaks')], keys_exclude=['ax'])\n",
    "    y = model_pdf(x, **kw_pars)\n",
    "    li, = ax.plot(x, y, lw=2, color='k')\n",
    "grid.add_legend(legend_data=dict(onepeak=li1, twopeaks=li), \n",
    "                label_order=['onepeak', 'twopeaks'], title='model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that `FacetGrid` only takes one DataFrame as input. In the previous\n",
    "example we provide the DataFrame of \"experimental\" data (`ds`) and use the `.map` method to plot\n",
    "histograms of the different datasets. The fitted distributions, instead, are\n",
    "plotted manually in the for loop.\n",
    "\n",
    "We can invert the approach, and pass to `FacetGrid` the DataFrame of fitted parameters (`dt_tot`),\n",
    "while leaving the simple histogram for manual plotting. In this case we need to write an \n",
    "helper function (`_plot`) that knows how to plot a distribution given a set of parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _plot(names, values, x, label=None, color=None):\n",
    "    df = pd.concat([names, values], axis=1)\n",
    "    kw_pars = br.tidy_to_dict(df, keys_exclude=['ax'])\n",
    "    func = model_pdf1 if label == 'onepeak' else model_pdf\n",
    "    y = func(x, **kw_pars)\n",
    "    plt.plot(x, y, lw=2, color=color, label=label)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bins = np.arange(-5, 5.01, 0.25)\n",
    "x = bins[:-1] + 0.5*(bins[1] - bins[0])\n",
    "grid = sns.FacetGrid(dt_tot.query('dataset < 6'), col='dataset', hue='model', col_wrap=3)\n",
    "grid.map(_plot, 'name', 'value', x=x)\n",
    "grid.add_legend()\n",
    "for i, ax in enumerate(grid.axes):\n",
    "    ax.hist(ds.query('dataset == %d' % i).data, bins=bins, histtype='stepfilled', normed=True, \n",
    "            color='gray', alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an even better (i.e. simpler) example of plots of fit results see\n",
    "[pybroom-example-multi-datasets](pybroom-example-multi-datasets.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
